### 损失函数的梯度

#### MSE的梯度

```python
x = tf.random.normal([2, 4])
w = tf.random.normal([4, 3])
b = tf.zeros([3])
y = tf.constant([2, 0])  # 这就是label
label = tf.one_hot(y, depth=3)

with tf.GradientTape() as tape:
    tape.watch([w, b])  # 如果w和b是tf.Varialble就会自动跟踪，如果不是就需要watch
    prob = tf.nn.softmax(x@w+b, axis=1)
    loss = tf.reduce_mean(tf.losses.MSE(label, prob))

grads = tape.gradient(loss, [w, b])
print('w:', grads[0])
print('b:', grads[1])
```

#### 交叉熵的梯度

```python
x = tf.random.normal([2, 4])
w = tf.random.normal([4, 3])
b = tf.zeros([3])
y = tf.constant([2, 0])  # 这就是label
label = tf.one_hot(y, depth=3)

with tf.GradientTape() as tape:
    tape.watch([w, b ])
    logits = x@w+b  # 这里不需要放入一个激活函数
    loss = tf.reduce_mean(tf.losses.categorical_crossentropy(label, logits, from_logits=True))

grads = tape.gradient(loss, [w, b])
print('w:', w)
print('b:', b)
```



#### 为什么要使用损失函数的梯度

不管是MSE还是交叉熵的损失函数，都是越接近于零，也就是越小正确率越高。为了让loss函数达到最小，就需要利用梯度来不断地更新w和b等参数，使得loss减小。达到提高准确率的效果。