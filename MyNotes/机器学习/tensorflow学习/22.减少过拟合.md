## 减少过拟合

#### 减少过拟合的方式

- 增加数据

- 数据增强&噪声数据

- 简化模型

- 提前终止

- 正则化

  >Occam's Razor 原理，奥卡姆剃刀，又称“奥坎的剃刀”，拉丁文为lex parsimoniae，意思是简约之法则，是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉提出的一个解决问题的法则，他在《箴言书注》2卷15题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。

  正则化的一个最强大最知名的特性就是能向损失函数增加“惩罚项”（penalty）。所谓『惩罚』是指对损失函数中的某些参数做一些限制，减少模型复杂度。最常见的惩罚项是L1和L2：

  - L1正则

    L1惩罚项的目的是将权重的绝对值最小化

  - L2正则
  
    L2惩罚项的目的是将权重的平方值最小化

#### 解决方法

1. Regularization 正则化

   - L1-regularization
     $$
     J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y_i\ln\hat{y_i}+(1-y_i)\ln(1-\hat{y_i})]-\lambda\sum_{i=1}^n|\theta_i|
     $$

   - L2-regularization
     $$
     J(W;X,y)+\frac{1}{2}\lambda\mid\mid{W}\mid\mid^2
     $$
     TensorFlow中使用L2

     ```python
         L2_model = keras.models.Sequential([
             keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),#0.002是 lambda
                                 activation=tf.nn.reul, input_shape=(NUM_WORDS,)),
             keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                                 activation=tf.nn.reul, input_shape=(NUM_WORDS,)),
             keras.layers.Dense(1, activation=tf.nn.sigmoid)
                                 
         ])
     ```

     或者如下

     ```python
     with tf.GradientTape() as tape:
         # [b, 28, 28] => [b, 784]
         x = tf.reshape(x, (-1, 28*28))
         # [b, 784] => [b, 10]
         out = network(x)
         # [b] => [b, 10]
         y_onehot = tf.one_hot(y, depth=10) 
         # [b]
         loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))
     
     		# 进行正则化
         loss_regularization = []
         for p in network.trainable_variables:
             loss_regularization.append(tf.nn.l2_loss(p))
         loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))
     
         loss = loss + 0.0001 * loss_regularization
     
     
     grads = tape.gradient(loss, network.trainable_variables)
     optimizer.apply_gradients(zip(grads, network.trainable_variables))
     ```

     

2. Early stopping

3. Dropout

   - Learning less to learn better

   - Each connection has $p$ = [0, 1] to lose

     ![](https://tva1.sinaimg.cn/large/006tNbRwly1gbltj05bpdj30wa0g6ajx.jpg)


   在需要的地方添加`Dropout`层，可以指定参数断掉的概率

   ```python
   network = Sequential([layers.Dense(256, activation='relu'),
                        layers.Dropout(0.5), # 0.5 rate to drop
                        layers.Dense(128, activation='relu'),
                        layers.Dropout(0.5), # 0.5 rate to drop
                        layers.Dense(64, activation='relu'),
                        layers.Dense(32, activation='relu'),
                        layers.Dense(10)]
   ```

   使用了dropout需要在训练和测试的时候添加一个参数

   ```python
   with tf.GradientTape() as tape:
       # [b, 28, 28] => [b, 784]
       x = tf.reshape(x, (-1, 28*28))
       # [b, 784] => [b, 10]
       out = network(x, training=True)  # 这个参数在训练时需要调整为True
       y_onehot = tf.one_hot(y, depth=10) 
       # [b]
   		loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True)
       # ....
                             
       # test
       for step, (x, y) in enumerate(ds_val): 
           # [b, 28, 28] => [b, 784]
           x = tf.reshape(x, (-1, 28*28))
           # [b, 784] => [b, 10] 
           out = network(x, training=False)  # 这个参数在测试时需要调整为False
           # [b, 10] => [b] 
           pred = tf.argmax(out, axis=1) 
                            
   ```
   



