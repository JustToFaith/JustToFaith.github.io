

## 循环神经网络

1. 什么是RNN

   RNN背后的想法是利用顺序信息。在传统的神经网络中，我们假设所有输入（和输出）彼此独立。但是对于许多任务来说，这是一个非常糟糕的主意。如果您想预测句子中的下一个单词，您最好知道哪个单词在它之前。RNN之所以称为*递归，* 是因为它们对序列的每个元素执行相同的任务，其输出取决于先前的计算。思考RNN的另一种方法是，它们具有“内存”，可以捕获有关到目前为止已计算出的内容的信息。理论上，RNN可以按任意长的顺序使用信息，但实际上，它们仅限于回顾一些步骤（稍后再介绍）。这是典型的RNN的样子：

   ![](http://images2015.cnblogs.com/blog/947235/201608/947235-20160821234331464-1137952568.png)

   上图显示了将RNN *展开*（或展开）到完整的网络中。通过展开，我们仅表示我们为整个序列写出了网络。例如，如果我们关心的序列是5个单词的句子，则该网络将展开为5层神经网络，每个单词一层。控制RNN中发生的计算的公式如下：

   - ![_](http://s0.wp.com/latex.php?zoom=2&latex=x_t&bg=ffffff&fg=000&s=0)是时间步的输入![Ť](http://s0.wp.com/latex.php?zoom=2&latex=t&bg=ffffff&fg=000&s=0)。例如，![x_1](http://s0.wp.com/latex.php?zoom=2&latex=x_1&bg=ffffff&fg=000&s=0) 可以是与句子的第二个单词相对应的一个热门向量。
   - ![s_t](http://s0.wp.com/latex.php?zoom=2&latex=s_t&bg=ffffff&fg=000&s=0)是时间步的隐藏状态![Ť](http://s0.wp.com/latex.php?zoom=2&latex=t&bg=ffffff&fg=000&s=0)。这是网络的“内存”。![s_t](http://s0.wp.com/latex.php?zoom=2&latex=s_t&bg=ffffff&fg=000&s=0)根据先前的隐藏状态和当前步骤的输入来计算![s_t = f（Ux_t + Ws_ {t-1}）](http://s0.wp.com/latex.php?zoom=2&latex=s_t%3Df%28Ux_t+%2B+Ws_%7Bt-1%7D%29&bg=ffffff&fg=000&s=0)。该函数![F](http://s0.wp.com/latex.php?zoom=2&latex=f&bg=ffffff&fg=000&s=0)通常是诸如[tanh](https://reference.wolfram.com/language/ref/Tanh.html)或[ReLU之](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))类的非线性函数。  ![s _ {-1}](http://s0.wp.com/latex.php?zoom=2&latex=s_%7B-1%7D&bg=ffffff&fg=000&s=0)计算第一个隐藏状态所需的，通常会初始化为全零。
   - ![o_t](http://s0.wp.com/latex.php?zoom=2&latex=o_t&bg=ffffff&fg=000&s=0)是步骤的输出![Ť](http://s0.wp.com/latex.php?zoom=2&latex=t&bg=ffffff&fg=000&s=0)。例如，如果我们想预测句子中的下一个单词，那么它将是整个词汇表中概率的向量。![o_t = \ mathrm {softmax}（Vs_t）](http://s0.wp.com/latex.php?zoom=2&latex=o_t+%3D+%5Cmathrm%7Bsoftmax%7D%28Vs_t%29&bg=ffffff&fg=000&s=0)。

2. 结构

   ![](https://tva1.sinaimg.cn/large/0082zybply1gbozsn97qaj31jc0r0n1u.jpg)

   $h_0$ 一般初始化全部赋值为0[0,0,...,0]

3. 计算的过程

   ![](https://tva1.sinaimg.cn/large/0082zybply1gbozx106m6j31km0u0tug.jpg)

4. 代码

   ```python
   In [4]: cell = layers.SimpleRNNCell(3)
   
   In [5]: cell.build(input_shape=(None, 4))  # None是batch，4是输入的维度
   
   In [6]: cell.trainable_variables
   Out[6]:
   [<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=
    array([[-0.76613724,  0.15255916, -0.38968062],
           [-0.38561964, -0.5816579 , -0.26669765],
           [ 0.48427188, -0.4897973 , -0.40993154],
           [ 0.7460693 , -0.14529377, -0.35444772]], dtype=float32)>,
    <tf.Variable 'recurrent_kernel:0' shape=(3, 3) dtype=float32, numpy=
    array([[ 0.87889934,  0.04475823,  0.47490296],
           [ 0.29954213, -0.8265981 , -0.47645584],
           [ 0.37122858,  0.5610101 , -0.7399034 ]], dtype=float32)>,
    <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]
   ```

   `kernel:0`是$W_{xh}$ =[input, h] ，`recurrent_kernel:0`是$W_{hh}$=[h, h]

   **SimpleRNNCell**

   - $out,h_1=call(x, h_0)$  out和h1是同一个数值，但是h1是一个`list`
     - x：[b, seq len, word vec]
     - $h_0/h_1：[b, h dim]$
     - out：[b, h dim]
   
5. LSMT

   可以在一定程度上解决梯度弥散的问题。

6. GRU

   GRU和LSMT的预测效果差不多，但是GRU比LSMT少一个“门控”，简便了算法，所以相对于LSMT来说节约了计算成本和时间成本。

---

相关文章：

[RNN结构详解](https://www.jiqizhixin.com/articles/2018-12-14-4)

[递归神经网络教程](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

