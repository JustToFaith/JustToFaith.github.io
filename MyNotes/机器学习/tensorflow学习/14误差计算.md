### 误差计算

### Outline

- MSE  均方差
- Cross Entropy Loss  交叉熵损失
- Hinge Loss

#### Content

1. MSE

   - loss = $\frac{1}{N}\sum(y - out)^2$

     N一般是bias

   - 二范数：$L_2-norm = \sqrt{\sum(y - out)^2} $

   ```python
   y = tf.constant([1, 2, 3, 0, 2])
   y = tf.one_hot(y, depth=4)
   y = tf.cast(y, dtype=tf.float32)
   
   out = tf.random.normal([5, 4])
   
   loss = tf.reduce_mean(tf.square(y-out))  # ((y-out)^2)/N
   loss2 = tf.square(tf.norm(y -out))/(4*5)  # 将2范数转换到loss
   loss3 = tf.reduce_mean(tf.losses.MSE(y, out))
   loss4 = tf.reduce_mean(tf.losses.MSE(out, y))
   # MSE返回的是一个shape为batch(数的)的tensor，
   
   print('loss:', loss)
   print('loss2:', loss2)
   print('loss3:', loss3)
   print('loss4:', loss4)
   
   #输出
   loss: tf.Tensor(1.3771045, shape=(), dtype=float32)
   loss2: tf.Tensor(1.3771045, shape=(), dtype=float32)
   loss3: tf.Tensor(1.3771045, shape=(), dtype=float32)
   loss4: tf.Tensor(1.3771045, shape=(), dtype=float32)
   ```

2. Cross Entropy Loss

   - Entropy  熵

     $Entropy=-\sum P(i)logP(i)$

     lower entropy $\to$ more info  熵越小，信息量越大，越不稳定。

     ```python
     a = tf.fill([4], 0.25)
     m = a*tf.math.log(a)/tf.math.log(2.)
     m2 = -tf.reduce_sum(m)
     print('熵：', m2)
     # 输出
     熵： tf.Tensor(2.0, shape=(), dtype=float32)
     # 熵比较大，信息量少，比较稳定
     
     a = tf.constant([0.01, 0.01, 0.01, 0.97])
     m = a*tf.math.log(a)/tf.math.log(2.)
     m2 = -tf.reduce_sum(m)
     print('熵：', m2)
     # 输出
     熵： tf.Tensor(0.24194068, shape=(), dtype=float32)
     #将概率变得很不均匀，熵变小，不稳定
     ```

   - Cross Entropy

     $H(p,q)=-\sum p(x)logq(x)$

     $H(p,q)=H(p)+D_{KL}(p|q)$  

     当p为`one-hot`格式时，$H(p)=0$,上面的式子可以变为$H(p,q)=D_{KL}(p|q)=-1logq_1$ 

     如果p=q，则$D_{KL}(p|q)$=0。即如果loss=0(q=p时)则预测的分布和真实的分布一样，w是最好的状态。

     - 二分类问题

       ![](https://tva1.sinaimg.cn/large/006tNbRwly1gbh3aoljt0j31an0fjgr0.jpg)

       二分类的问题有两种设计格式，有两个输出或者有一个输出。前者输出0和1，后者只输出1。

       1. Single Output  

          $H(P,Q)=-P(cat)logQ(cat)-(1-P(cat))log(1-Q(cat))$

            $P(dog)=(1-P(cat))$

          $H(P,Q) = \displaystyle \sum_{i=(cat,dog)} P(i)logQ(i)$

          ​                $ =-P(cat)logQ(cat)-P(dog)logQ(dog)$

          ​                $=-(ylog(p)+(1-y)log(1-p))$

          

          $\therefore loss=-(ylog(p)+(1-y)log(1-p))$

          `tf.losses.binary_crossentropy()`

          ```python
          In [7]: tf.losses.binary_crossentropy([1],[0.1])
          Out[7]: <tf.Tensor: shape=(), dtype=float32, numpy=2.3025842>
          # 预测错误的概率比较高，loss相对较大
          ```

          

       2. Classification

          - $H([0,1,0],[p_0,p_1,p_2])=0+D_{KL}(p|q)=-1logq_1$

            举例：

            ![](https://tva1.sinaimg.cn/large/006tNbRwly1gbh4s8d7vgj31gg0u0wv4.jpg)

            如果预测正确的概率很大，loss接近于0。P为lable，Q为预测的概率。

            在`Tensorflow`中使用`tf.losses.categorical_crossentropy()`

            ```python
            In [6]: tf.losses.categorical_crossentropy([0, 1, 0, 0],[0.01, 0.97, 0.01, 0.01])
            Out[6]: <tf.Tensor: shape=(), dtype=float32, numpy=0.030459179>
            # 预测正确的概率比较高，loss较小
            ```

            

