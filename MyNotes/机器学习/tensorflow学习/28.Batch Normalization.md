## Batch Normalization

1. 为什么要batch normalization

   在深度学习中，我们通常为了有个很好的训练效果，需要增加网络的层数。我们大家都知道，由于深度学习网络层与网络层之间存在较高度的关联性和耦合性，所以深度学习是难以训练的。并且一方面由于每层网络之间关联性太强，前方网络层中一些微弱的变化会导致后方网络层的相续变化，并且这种变化的影响在一定程度上会大。另一方面每层参数的不断变化导致每一层的输入分布会发生改变进而网络需要不停地去适应这些分布变化，使得我们的模型训练变得异常困难。这种现象叫做Internel Covariate Shite。

2. 什么是batch normalization

   用自己简单的话来说就是，由于一些变化，导致使输入`sigmoid`函数的中值向极限饱和区靠拢(也就是函数值靠近0或1的部分)，在这区域的函数值的导数很小，更新缓慢。为了防止这种情况发生，我们人为让其特征分布的函数的均值为0，方差为1，让在`sigmoid`函数中的输入值在0附近，那里的函数值导数大，变化明显。

---

相关好文：

[Batch Normalization原理与实战](https://zhuanlan.zhihu.com/p/34879333)

[深入理解Batch Normalization批标准化](https://www.cnblogs.com/guoyaohua/p/8724433.html)

