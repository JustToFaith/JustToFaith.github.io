## 自定义层或网络

#### Outline

- keras.Sequential
- keras.layers.Layer
- keras.Model

#### Content

我们想使用Sequential来简易地调用多层结构和使用其他好用的接口，但是又想灵活地使用各种方法。这时就可以自定义网络层来实现。

1. keras.Sequential

   首先是初始化Sequential，和之前的步骤一样。

   ```python
   network = Sequential([layers.Dense(256, activation='relu'),
                        layers.Dense(128, activation='relu'),
                        layers.Dense(64, activation='relu'),
                        layers.Dense(32, activation='relu'),
                        layers.Dense(10)])
   network.build(input_shape=(None, 28*28))
   network.summary()
   ```

2. keras.layers.Layer

   自定义layer层，它继承于`layers.Layer`。

   ```python
   class MyDense(layers.Layer):
   	
   	def __init__(self, inp_dim, outp_dim):  # 初始化
   		super(MyDense, self).__init__()  # 调用母类的初始化方法
   		
       # 下面两个都是可以自己发挥的地方，也就是可以更改的
   		self.kernel = self.add_variable('w', [inp_dim, outp_dim])  # variable改成weight好像也可以
   		self.bias = self.add_variable('b', [outp_dim])
   
   	def call(self, inputs, training=None):
   
   		out = inputs @ self.kernel + self.bias
   
   		return out 
   ```

3. keras.Model

   自定义网络，它继承与`keras.Model`。

   ```python
   class MyModel(keras.Model):
   
   	def __init__(self):  # 初始化
   		super(MyModel, self).__init__()
   		# 创建的层，使用自定义层
   		self.fc1 = MyDense(28*28, 256)
   		self.fc2 = MyDense(256, 128)
   		self.fc3 = MyDense(128, 64)
   		self.fc4 = MyDense(64, 32)
   		self.fc5 = MyDense(32, 10)
   
   	def call(self, inputs, training=None):
   		# 在这里可以对x进行额外的操作，比如使用relu函数或者其他操作等
   		x = self.fc1(inputs)
   		x = tf.nn.relu(x)
   		x = self.fc2(x)
   		x = tf.nn.relu(x)
   		x = self.fc3(x)
   		x = tf.nn.relu(x)
   		x = self.fc4(x)
   		x = tf.nn.relu(x)
   		x = self.fc5(x) 
   
   		return x
   ```

   最后将MyModel赋给network

   ```python
   network = MyModel()
   ```

   

