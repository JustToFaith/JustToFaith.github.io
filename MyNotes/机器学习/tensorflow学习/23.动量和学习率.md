## 动量和学习率

#### 动量

```python
optimizer = SGD(learning_rate=0.02, momentum=0.9)
optimizer = RMSprop(learning_rate=0.02, momentum=0.9)

optimizer = Adam(learning_rate=0.02, beta_1=0.9, beta_2=0.999)  # adam好像已经包含动量了
```





#### 学习率

让learning_rate自动微调，开始的时候很大，之后慢慢变小。

```python
optimizer = SGD(learning_rate=0.2)

for epoch in range(100):
    
    # get loss

    # change learing rate
    optimizer.learning_rate = 0.2 * (100-epoch)/100

    # update weights
```

