### 梯度下降

#### Outline

- What's Gradient
- What does it mean
- How to Search
- AutoGrad

#### Content

1. What's Gradient

   - 导数，derivative

   - 偏微分，partial derivative

     $z=y^2-x^2$

     $\frac{\partial{z}}{\partial{x}}=-2x$

     $\frac{\partial{z}}{\partial{y}}=2y$ 

   - 梯度，gradient

     $\nabla f=(\frac{\partial{f}}{\partial{x_1}};\frac{\partial{f}}{\partial{x_2}};...;\frac{\partial{f}}{\partial{x_n}})$

   导数是一个通用的概念，一个函数在某一点的导数描述了这个函数在这一点附近的变化率；

   偏微分是一个多元函数对不同的变量取其导数；~~自己的傻逼理解~~

   梯度是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度方向）变化最快。

2. AutoGrad

   自动求梯度用`with tf.GradientTape() as tape`

   - with tf.GradientTape() as tape:
     - Build computation graph
     - $loss=f_\theta(x)$
   - [w_grad]=tape.gradient(loss, [w])

   ```python
   w = tf.constant(1.)
   x = tf.constant(2.)
   y=x*w
   
   with tf.GradientTape() as tape:
       tape.watch(([w]))
       y2 = x*w
   grad = tape.gradient(y2,[w])  # 把w的括号去掉之后，grad的格式发生变化
   print('w:', grad)
   # 输出
   w: [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>]
   ```

   **注意**

   如果把8行的y2换成y，输出的结果是None。必须把式子放在`with`里面

   把8行的w换成x也不行，输入为None。因为x并没有在`with`里面被追踪。

   `tape.gradient(y,[w])`只能被调用一次，如果想要多次重复调用需要改变第五行`tf.GradientTape(persistent=True)`。用此参数后在调用完`tape.gradient(y,[w])`会自动释放资源。

   ##### 二阶求导

   ```python
   with tf.GradientTape() as t1:
     with tf.GradientTape() as t2:
       y = x * w + b
     dy_dw, dy_db = t2.gradient(y, [w, b])
   d2y_dw2 = tf.gradient(dy_dw, [w])
   ```

   

